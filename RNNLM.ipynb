{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95cd055-ded9-4aba-a405-a224731b4e9f",
   "metadata": {},
   "source": [
    "# This notebook contains an implementation for RNN-based language modelling. \n",
    "At the core, a language model is a sequence classifier that uses all the tokens produced so far as input in order to produce a probability density function over all possible next tokens (a token could be a word, a character, or something inbetween). We can then either use the \"best possible guess\" of the classifier as the next token, or we can sample from the candidates according to the likelihood distribution specified by the classifier. People also often manipulate the distribution before sampling, by multiplying values with a given *model temperature*. We do not implement this here.\n",
    "\n",
    "In fact, producing a probability density function comes for free, when we build a neural classifier that uses a softmax output activation. Therefore, nothing actually changes from \"before\", when we simply built classifiers. Once we have trained the model, we can repeatedly ask for next tokens, and add these to the context / the state of the model. This is called \"autoregressive sequence generation\".\n",
    "\n",
    "However, training as before is quite inefficient for an RNN-based language model. Specifically, we do not want to re-encode the full input for each prediction of the next token. Instead, we combine prediction of next token and forced setting of the correct next token for each full sequence. This makes the training more efficient, but the code more difficult to read. (And it is still very slow...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdfcf22-1179-4413-a94c-12846087c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import ipywidgets as widgets\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09659109-f01c-44ca-99a4-8bb11161277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '#', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xad', '½', 'Ä', 'É', 'Ö', 'Ü', 'ß', 'á', 'ä', 'ç', 'è', 'é', 'ê', 'ï', 'ò', 'ó', 'ô', 'ö', 'ú', 'ü', 'ă', 'ć', 'ę', 'ğ', 'ł', 'ń', 'ō', 'ř', 'ś', 'ž', '‐', '–', '‘', '’', '‚', '“', '”', '„', '…', '<s>', '</s>']\n",
      "['Liebe Mitbürgerinnen und Mitbürger, jetzt geht es los.', 'Der Anstoß zur Fußball-Weltmeisterschaft steht unmittelbar bevor.', 'Millionen haben auf diesen Augenblick gewartet - nicht nur in Deutschland, sondern in der ganzen Welt.', 'Vor dem Eröffnungsspiel gegen Costa Rica bin ich noch einmal mit Jürgen Klinsmann und unserer Nationalmannschaft zusammengetroffen.']\n"
     ]
    }
   ],
   "source": [
    "# load some text data; we'll try to model that below (or simpler alternatives)\n",
    "\n",
    "START_SYMBOL = \"<s>\"\n",
    "END_SYMBOL = \"</s>\"\n",
    "\n",
    "data = open('data/merkel-de.txt', 'r').read() # should be simple plain text file\n",
    "characters = set(data)\n",
    "characters = list(sorted(characters))\n",
    "characters.append(START_SYMBOL)\n",
    "characters.append(END_SYMBOL)\n",
    "characters.remove('\\n')\n",
    "NUM_CHARACTERS = len(characters)\n",
    "sentences = data.splitlines()\n",
    "int2char = list(characters)\n",
    "char2int = {c:i for i,c in enumerate(characters)}\n",
    "print(characters)\n",
    "print(sentences[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d4a1b19-8640-479a-92ea-28c6fcecc1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = NUM_CHARACTERS\n",
    "EMBED_SIZE = 32\n",
    "HIDDEN_SIZE = 64\n",
    "LAYERS = 2\n",
    "MAX_GENERATION_LENGTH = 400\n",
    "# okay, what's a recurrent neural network anyway? see https://calvinfeng.gitbook.io/machine-learning-notebook/supervised-learning/recurrent-neural-network/recurrent_neural_networks\n",
    "NUM_CLASSES = NUM_CHARACTERS\n",
    "\n",
    "class LM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LM, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(INPUT_SIZE, EMBED_SIZE)\n",
    "        self.rnn = nn.LSTM(EMBED_SIZE, HIDDEN_SIZE, LAYERS)\n",
    "        self.final_layer = nn.Linear(HIDDEN_SIZE, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, xs : torch.tensor):\n",
    "        xs = self.embed(xs)\n",
    "        rnn_outputs, _ = self.rnn(xs)\n",
    "        results = nn.functional.log_softmax(self.final_layer(rnn_outputs), dim=1)\n",
    "        return results\n",
    "\n",
    "    def forwardx(self, xs : torch.tensor):\n",
    "        xs = self.embed(xs)\n",
    "        h_n = torch.zeros(LAYERS, HIDDEN_SIZE)\n",
    "        rnn_outputs = []\n",
    "        for x in xs:\n",
    "            x = x[None,:]\n",
    "            rnn_output, h_n = self.rnn(x, h_n)\n",
    "            rnn_outputs.append(rnn_output)\n",
    "        rnn_outputs = torch.cat(rnn_outputs)\n",
    "        results = nn.functional.log_softmax(self.final_layer(rnn_outputs), log=1)\n",
    "        return results\n",
    "\n",
    "    def generate(self, xs=torch.tensor([char2int[START_SYMBOL]]), sample=\"max\") -> torch.tensor:\n",
    "        \"\"\"sample can be \"max\" or \"prop\" for max likelihood or proportional sampling\"\"\"\n",
    "        classification = None\n",
    "        h_n = None\n",
    "        output = []\n",
    "        xs = self.embed(xs)\n",
    "        while ((classification == None) or (classification.item() != char2int[END_SYMBOL])) and (len(output) < MAX_GENERATION_LENGTH):\n",
    "            rnn_outputs, h_n = self.rnn(xs, h_n)\n",
    "            if sample == \"max\":\n",
    "                classification = torch.argmax(self.final_layer(rnn_outputs[-1]))\n",
    "            elif sample == \"prop\":\n",
    "                classification = torch.multinomial(nn.functional.softmax(self.final_layer(rnn_outputs[-1]), dim=0), 1)[0]\n",
    "            else:\n",
    "                assert False, \"only max and prop are possible values for sample!\"\n",
    "            output.append(classification)\n",
    "            xs = self.embed(classification)[None,:]\n",
    "        output = torch.stack(output[:-1]) if len(output) > 1 else torch.tensor([])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa9d244-e2f1-4f2a-9317-285610cb3654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 starting\n",
      "forced: Dch wirl  dass iiele Grfeitseren dendeceeisann  ianen  der dinder distnnt  dissich nn drgendstneenttittn   dht egieilienssansce dnd drail gee tidadnd dst disit din ueseegedür dis snternehren \n",
      "freemax:ensend sie der einen wir die Gesundheitsscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitscheitschei\n",
      "fresamp:vachze welter kchzeinivent und Miblangenigeg beweben den undeslürger EsUndwerhafter Pürgen aben Malt eg E-underm licht est flat Verung zu Ech den Erarmarn gangen\n",
      "Epoch 2 starting\n",
      "forced: Das wlterngeld  sas wir terane Veschlossen haben</s> dst ein uichtigen peit agszur besceren keraingarkeit aon derun dnd samitie \n",
      "freemax:empt.\n",
      "fresamp:pozk\n",
      "Epoch 3 starting\n",
      "forced: Uir aorden as dchaffen, dass die Veräntwortlichkeiten zwischen der oundestren  mnd den Bändern neu gefrgnet berden und drare  Vonesnander gebreubt werden, das meißt das  die Länderaginage Vnfgaben ku elertt gekommen</s>\n",
      "freemax:ümen\n",
      "fresamp:1Umets leichter kann können mährmach us beistag, zahat mehr Well.\n",
      "Epoch 4 starting\n",
      "forced: Das weißt zum Beispiel  dass beisferrerist schen Kesahren in Zukunft das Bundeskriminalamt koordinierend fitig sein kann</s> wenn die Gefahr über die eeenzen dines eineelnen wundesrantes,hinausgeht</s>\n",
      "freemax:ümenpoetge Ver oder auf den Prohlitteinivor vergangenen haben\n",
      "fresamp:heßßtsolmen Jahite Kinder Sotäle Versorgung\n",
      "Epoch 5 starting\n",
      "forced: Dae Bundesregierung hat Eckpunkte für eine neue Gesundheitsversocherung beschlossen</s>\n",
      "freemax:ich habeh. aus auft euf lat.\n",
      "fresamp:Gnedlich aufgankite zu Hebensjung zu erfalizieltung wall uns alten die mehr und St alie Verantwortlichkeiten können, wo das Gesundheitswesen muss durchschaubarer werden\n",
      "Epoch 6 starting\n",
      "forced: Uar Anstoß zuraFußballmWeltmeisterschaft steht unmictelbar bevor.</s>\n",
      "freemax:ich habet.\n",
      "fresamp:reKrab ,n dem Knit der in Geselnen und Schkkavenzu gereitswert parichlichen Eram Heigen haben.\n",
      "Epoch 7 starting\n",
      "forced: Das beißt, dass alle die bestmögliche Gesundheitsversorgung aekommen – egal, ob sie jung oder alt, egal, ob sie arm oder reich sind.</s>\n",
      "freemax:ich habet.\n",
      "fresamp:Vichzutlich ud weinstet in unserere Gesundheitswesen ist weltweit anerkannt\n",
      "Epoch 8 starting\n",
      "forced: Deute geht es lir um ein wichtiges Lrojekt, das wir am bittwech im Aundeskabinete beschlossen haben, das Eiterngeld.</s>\n",
      "freemax:ich habekensi der ob kenn kann, som.\n",
      "fresamp:Heußgetojgt von auf dem Platz.\n",
      "Epoch 9 starting\n",
      "forced: Deder einzelle Spieler ist hochmotiviert und wird - davon bin ich fest überzeugt - sein Bestes geben.</s>\n",
      "freemax:ich kunfrent -äh Versicherten besser auf in den Unternehmen mit Zukunft\n",
      "fresamp:ünm dem Ihrechnest eine überzen, was diese Vie dies Unterntehren sind Unternehmen mit Zukunfnwinverd.\n",
      "Epoch 10 starting\n",
      "forced: Uphreiben Sie mir Ihre Meinung per E-Mail.</s>\n",
      "freemax:ich habek.\n",
      "fresamp:Gnim der Medizin und der Medizintechnik\n",
      "Epoch 11 starting\n",
      "forced: Dm degenzug bekommt der Bund neue Verantwortlichkeiten oder alleinige Verantwortlichkeiten</s>\n",
      "freemax:ich: ist indender neine neu der wichtigsten Zukunftsbranchen.\n",
      "fresamp:Gnder sich auch.\n",
      "Epoch 12 starting\n",
      "forced: Dar längere Lebensabend bedeutet naturgemäß eine deutlich höhere Belastung für die Krankenkassen.</s>\n",
      "freemax:ich: inassnit alneld.\n",
      "fresamp:„engtzakren uns auf VatsEch bekommen, anen, Deutsworder kann kungeren selbst beantworten kann: Ihre Meinung zählt - darauf können Sie sich auf dhereitrübscheichere Vierungen wie zum Beispiel die Gaststättenregelungen, das Versammlungsrecht, die Arbeitskenfarstreht vergantworten ihmen und das heich die Gesundheitsreform\n",
      "Epoch 13 starting\n"
     ]
    }
   ],
   "source": [
    "training_data = [\"hello\"] * 10\n",
    "#training_data = [\"abcdefghijklmnopqrstuvwxyz\"] * 20\n",
    "#training_data = [\"Möglicherweise haben Sie bei einem Fußballspiel schon einmal etwas von einer Bananenflanke gehört.\"] * 50\n",
    "#training_data = sentences[0:100] * 50\n",
    "MAX_EPOCHS = 30\n",
    "\n",
    "def to_vector(sentence : str, noend=False) -> torch.tensor:\n",
    "    sentence = [START_SYMBOL] + list(sentence)\n",
    "    if not noend:\n",
    "        sentence.append(END_SYMBOL)\n",
    "    return torch.tensor([char2int[c] for c in sentence])\n",
    "\n",
    "lm = LM()\n",
    "optimizer = torch.optim.Adam(lm.parameters())\n",
    "\n",
    "def training(training_data, validation_data=[]):\n",
    "    training_data = [to_vector(s) for s in training_data]\n",
    "    validation_data = [to_vector(s) for s in validation_data]\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        print((\"Epoch {} starting\".format(epoch+1)))\n",
    "        random.shuffle(training_data)\n",
    "        for s in training_data:\n",
    "            optimizer.zero_grad()\n",
    "            all_input = s[:-1]\n",
    "            all_targets = s[1:]\n",
    "            outputs = lm(all_input)\n",
    "            losses = nn.functional.nll_loss(outputs, all_targets)\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "        print(\"forced: \" + \"\".join([int2char[x] for x in torch.argmax(lm(training_data[0][:-1]), dim=1)]))\n",
    "        print(\"freemax:\" + \"\".join([int2char[x] for x in lm.generate()]))\n",
    "        print(\"fresamp:\" + \"\".join([int2char[x] for x in lm.generate(sample=\"prop\")]))\n",
    "    return lm\n",
    "\n",
    "\n",
    "lm = training(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1745ddea-c96d-4a39-ab26-63cefd26348f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
