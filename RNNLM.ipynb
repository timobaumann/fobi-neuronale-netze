{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95cd055-ded9-4aba-a405-a224731b4e9f",
   "metadata": {},
   "source": [
    "# This notebook contains an implementation for language modelling. \n",
    "At the core, a language model is a sequence classifier that uses all the tokens produced so far as input in order to produce a probability density function over all possible next tokens (a token could be a word, a character, or something inbetween). We can then either use the \"best possible guess\" of the classifier as the next token, or we can sample from the distribution according to the distribution. \n",
    "\n",
    "In fact, producing a probability density function comes for free, when we build a neural classifier that uses a softmax output activation. Therefore, nothing actually changes from \"before\", when we simply built classifiers.\n",
    "\n",
    "Once we have trained the model, we repeatedly ask for next tokens, and add these to the context. This is called \"autoregressive sequence generation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ccdfcf22-1179-4413-a94c-12846087c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import ipywidgets as widgets\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "09659109-f01c-44ca-99a4-8bb11161277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '#', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xad', '½', 'Ä', 'É', 'Ö', 'Ü', 'ß', 'á', 'ä', 'ç', 'è', 'é', 'ê', 'ï', 'ò', 'ó', 'ô', 'ö', 'ú', 'ü', 'ă', 'ć', 'ę', 'ğ', 'ł', 'ń', 'ō', 'ř', 'ś', 'ž', '̈', '‐', '–', '‘', '’', '‚', '“', '”', '„', '…', '<s>', '</s>']\n",
      "['Liebe Mitbürgerinnen und Mitbürger, jetzt geht es los. Der Anstoß zur Fußball-Weltmeisterschaft steht unmittelbar bevor. Millionen haben auf diesen Augenblick gewartet - nicht nur in Deutschland, sondern in der ganzen Welt.', 'Vor dem Eröffnungsspiel gegen Costa Rica bin ich noch einmal mit Jürgen Klinsmann und unserer Nationalmannschaft zusammengetroffen. Jeder einzelne Spieler ist hochmotiviert und wird - davon bin ich fest überzeugt - sein Bestes geben. Und das sollten auch wir tun. Sie wissen: Die Fans sind der zwölfte Mann auf dem Platz. Wir alle wollen zeigen, dass Deutschland zu Spitzenleistungen fähig ist.', 'Und das nicht nur in den Fußballstadien. Wir freuen uns auf Gäste aus allen Erdteilen und wollen mit ihnen ein großes Fest feiern. Friedlich und fröhlich. Und wir wollen ihnen zeigen, was dieses vielfältige Land zu bieten hat. Sportlich und auch kulturell. Ich freue mich sehr darüber, wie groß das Veranstaltungsangebot in den kommenden vier Wochen ist - weit über die offiziellen Ereignisse hinaus.', 'Das zeigt: Deutschland ist ein weltoffenes, ein modernes und ein lebendiges Land.']\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "START_SYMBOL = \"<s>\"\n",
    "END_SYMBOL = \"</s>\"\n",
    "\n",
    "data = open('data/merkel-de.txt', 'r').read() # should be simple plain text file\n",
    "characters = set(data)\n",
    "characters = list(sorted(characters))\n",
    "characters.append(START_SYMBOL)\n",
    "characters.append(END_SYMBOL)\n",
    "characters.remove('\\n')\n",
    "NUM_CHARACTERS = len(characters)\n",
    "sentences = data.splitlines()\n",
    "int2char = list(characters)\n",
    "char2int = {c:i for i,c in enumerate(characters)}\n",
    "print(characters)\n",
    "print(sentences[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1d4a1b19-8640-479a-92ea-28c6fcecc1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = NUM_CHARACTERS\n",
    "EMBED_SIZE = 16\n",
    "HIDDEN_SIZE = 64\n",
    "LAYERS = 4\n",
    "MAX_GENERATION_LENGTH = 800\n",
    "# okay, what's a recurrent neural network anyway? see https://calvinfeng.gitbook.io/machine-learning-notebook/supervised-learning/recurrent-neural-network/recurrent_neural_networks\n",
    "\n",
    "NUM_CLASSES = NUM_CHARACTERS\n",
    "class LM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LM, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(INPUT_SIZE, EMBED_SIZE)\n",
    "        torch.nn.init.xavier_normal_(self.embed.weight)\n",
    "        self.rnn = nn.GRU(EMBED_SIZE, HIDDEN_SIZE, LAYERS)\n",
    "        self.final_layer = nn.Linear(HIDDEN_SIZE, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, xs : torch.tensor):\n",
    "        xs = self.embed(xs)\n",
    "        rnn_outputs, _ = self.rnn(xs)\n",
    "        results = nn.functional.log_softmax(self.final_layer(rnn_outputs), dim=1)\n",
    "        return results\n",
    "\n",
    "    def forwardx(self, xs : torch.tensor):\n",
    "        xs = self.embed(xs)\n",
    "        h_n = torch.zeros(LAYERS, HIDDEN_SIZE)\n",
    "        #c_n = torch.zeros(LAYERS, HIDDEN_SIZE)\n",
    "        rnn_outputs = []\n",
    "        for x in xs:\n",
    "            x = x[None,:]\n",
    "            rnn_output, h_n = self.rnn(x, h_n)\n",
    "            rnn_outputs.append(rnn_output)\n",
    "        rnn_outputs = torch.cat(rnn_outputs)\n",
    "        results = nn.functional.log_softmax(self.final_layer(rnn_outputs), log=1)\n",
    "        return results\n",
    "\n",
    "    def generate(self, xs=torch.tensor([char2int[START_SYMBOL]]), sample=\"max\") -> torch.tensor:\n",
    "        \"\"\"sample can be \"max\" or \"prop\" for max likelihood or proportional sampling\"\"\"\n",
    "        classification = None\n",
    "        h_n = torch.zeros(LAYERS, HIDDEN_SIZE)\n",
    "        output = []\n",
    "        xs = self.embed(xs)\n",
    "        while ((classification == None) or (classification.item() != char2int[END_SYMBOL])) and (len(output) < MAX_GENERATION_LENGTH):\n",
    "            rnn_outputs, h_n = self.rnn(xs, h_n)\n",
    "            if sample == \"max\":\n",
    "                classification = torch.argmax(self.final_layer(rnn_outputs[-1]))\n",
    "            elif sample == \"prop\":\n",
    "                classification = torch.multinomial(nn.functional.softmax(self.final_layer(rnn_outputs[-1]), dim=0), 1)[0]\n",
    "            else:\n",
    "                assert False, \"only max and prop are possible values for sample!\"\n",
    "            output.append(classification)\n",
    "            xs = self.embed(classification)[None,:]\n",
    "        output = torch.stack(output[:-1]) if len(output) > 0 else torch.tensor([])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa9d244-e2f1-4f2a-9317-285610cb3654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 starting\n"
     ]
    }
   ],
   "source": [
    "#training_data = [\"hello\"] * 50\n",
    "#training_data = [\"abcdefghijklmnopqrstuvwxyz\"] * 300\n",
    "#training_data = [\"Möglicherweise haben Sie bei einem Fußballspiel schon einmal etwas von einer Bananenflanke gehört.\"] * 100\n",
    "training_data = sentences\n",
    "MAX_EPOCHS = 50\n",
    "\n",
    "def to_vector(sentence : str, noend=False) -> torch.tensor:\n",
    "    sentence = [START_SYMBOL] + list(sentence)\n",
    "    if not noend:\n",
    "        sentence.append(END_SYMBOL)\n",
    "    return torch.tensor([char2int[c] for c in sentence])\n",
    "\n",
    "lm = LM()\n",
    "optimizer = torch.optim.Adam(lm.parameters())\n",
    "\n",
    "def training(training_data, validation_data=[]):\n",
    "    training_data = [to_vector(s) for s in training_data]\n",
    "    validation_data = [to_vector(s) for s in validation_data]\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        print((\"Epoch {} starting\".format(epoch)))\n",
    "        random.shuffle(training_data)\n",
    "        for s in training_data:\n",
    "            optimizer.zero_grad()\n",
    "            all_input = s[:-1]\n",
    "            all_targets = s[1:]\n",
    "            outputs = lm(all_input)\n",
    "            losses = nn.functional.nll_loss(outputs, all_targets)\n",
    "            loss = torch.sum(losses)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"forced: \" + \"\".join([int2char[x] for x in torch.argmax(lm(training_data[0][:-1]), dim=1)]))\n",
    "        print(\"freemax:\" + \"\".join([int2char[x] for x in lm.generate()]))\n",
    "        print(\"fresamp:\" + \"\".join([int2char[x] for x in lm.generate(sample=\"prop\")]))\n",
    "    return lm\n",
    "\n",
    "\n",
    "lm = training(training_data)\n",
    "#result = lm(to_vector(\"\", True))\n",
    "#result = torch.topk(result, 3, dim=1)\n",
    "#print(\"\".join([int2char[x] for x in torch.argmax(result, dim=1)]))\n",
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80f39a-0ff2-4e97-b9b5-0ad78b7d984b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
