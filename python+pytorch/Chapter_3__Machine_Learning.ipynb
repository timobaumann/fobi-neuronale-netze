{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "Für Machine Learning gibt es eine Vielzahl von Bibliotheken. In diesem Kurs wollen wir uns auf die folgenden zwei beschränken:\n",
    "* scikit-learn\n",
    "* PyTorch\n",
    "\n",
    "\n",
    "## scikit-learn\n",
    "Scikit-learn ist eine Bibliothek für Machine Learning in Python. Sie ist einfach zu benutzen und baut auf NumPy, SciPy und matplotlib auf. Sie bietet eine Vielzahl von Algorithmen für Klassifikation, Regression, Clustering und Dimensionalitätsreduktion.\n",
    "\n",
    "Scikit-learn kann über den Paketmanager `pip` installiert werden:\n",
    "```bash \n",
    "pip install scikit-learn\n",
    "```\n",
    "Anschließend kann es in Python importiert werden:\n",
    "```python\n",
    "import sklearn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Datensätze\n",
    "Ein weiterer Vorteil von scikit-learn ist, dass auch ein Vielzahl von Datensätzen mitgeliefert werden. Eine Übersicht über die Datensätze findet sich [hier](https://scikit-learn.org/stable/datasets).\n",
    "\n",
    "#### Iris-Datensatz\n",
    "Der Iris-Datensatz ist ein Datensatz mit 150 Beobachtungen von drei verschiedenen Arten von Iris-Blumen (Setosa, Versicolour, und Virginica). Es gibt vier Eigenschaften (Features) für jede Beobachtung: Länge und Breite der Sepal und Länge und Breite der Petal. Der Datensatz ist in scikit-learn enthalten und kann wie folgt geladen werden:\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "```\n",
    "Der Datensatz ist ein Dictionary mit den folgenden Einträgen:\n",
    "* `data`: Die Daten als NumPy-Array\n",
    "* `target`: Die Zielvariablen als NumPy-Array\n",
    "* `target_names`: Die Namen der Zielvariablen als NumPy-Array\n",
    "* `feature_names`: Die Namen der Features als Liste\n",
    "* `DESCR`: Eine Beschreibung des Datensatzes\n",
    "* `filename`: Der Pfad zur Datei, in der der Datensatz gespeichert ist\n",
    "\n",
    "Die Daten können wie folgt in ein Pandas DataFrame geladen werden:\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "```\n",
    "Die Daten können dann wie gewohnt mit Pandas weiterverarbeitet werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handwritten Digits-Datensatz\n",
    "Der Handwritten Digits-Datensatz ist ein Datensatz mit 1797 Beobachtungen von handgeschriebenen Ziffern (0 bis 9). Jede Beobachtung ist ein Bild mit 8x8 Pixeln. Der Datensatz ist in scikit-learn enthalten und kann wie folgt geladen werden:\n",
    "```python\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "```\n",
    "Der Datensatz ist ein Dictionary mit den folgenden Einträgen:\n",
    "* `data`: Die Daten als NumPy-Array\n",
    "* `target`: Die Zielvariablen als NumPy-Array\n",
    "* `target_names`: Die Namen der Zielvariablen als NumPy-Array\n",
    "* `feature_names`: Die Namen der Features als Liste\n",
    "* `DESCR`: Eine Beschreibung des Datensatzes\n",
    "* `filename`: Der Pfad zur Datei, in der der Datensatz gespeichert ist\n",
    "\n",
    "Die Daten können wie folgt in ein Pandas DataFrame geladen werden:\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(digits.data, columns=digits.feature_names)\n",
    "df['target'] = digits.target\n",
    "```\n",
    "Die Daten können dann wie gewohnt mit Pandas weiterverarbeitet werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breast Cancer-Datensatz\n",
    "Der Breast Cancer-Datensatz ist ein Datensatz mit 569 Beobachtungen von Brustkrebs-Tumoren. Es gibt 30 Eigenschaften (Features) für jede Beobachtung: Radius, Textur, Umfang, Fläche, Glätte, Kompaktheit, Konkavität, Konkavitätspunkte, Symmetrie, Fraktale Dimension. Der Datensatz ist in scikit-learn enthalten und kann wie folgt geladen werden:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Titanic-Datensatz\n",
    "Der Titanic-Datensatz ist ein Datensatz mit 891 Beobachtungen von Passagieren der Titanic. Es gibt 12 Eigenschaften (Features) für jede Beobachtung: Klasse, Name, Geschlecht, Alter, Anzahl der Geschwister/Ehepartner an Bord, Anzahl der Eltern/Kinder an Bord, Ticketnummer, Ticketpreis, Kabine, Hafen, ob der Passagier überlebt hat. Der Datensatz ist in scikit-learn enthalten und kann wie folgt geladen werden:\n",
    "```python\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "titanic = fetch_openml(\n",
    "    \"titanic\", version=1, as_frame=True, return_X_y=False, parser=\"pandas\"\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klassifikation\n",
    "\n",
    "Zur besseren Veranschaulichung wollen wir die Klassifikation zunächst an niedrigdimensionalen dummy Daten demonstrieren. Dazu wurden bereits drei verschiedene Datensätze mit jeweils zwei Features (x1, x2) erstellt. Zwei der Datensätze weisen zwei und einer drei Klassen auf (siehe Abb.). \n",
    "\n",
    "<img src=\"images/sklearn/dummy_2_class_dataset.png\" width=\"350\">\n",
    "<img src=\"images/sklearn/dummy_3_class_dataset.png\" width=\"350\">\n",
    "<img src=\"images/sklearn/dummy_2_class_dataset_non-linear.png\" width=\"350\">\n",
    "\n",
    "\n",
    "Es sind deutlich die einzelnen Cluster der Klassen zu erkennen, die sich jedoch an den Grenzen überschneiden. \n",
    "In den folgenden Aufgaben sollen Sie verschiedene Klassifikationsalgorithmen auf diese Datensätze anwenden. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufgabe:\n",
    "# Lesen Sie den Datensatz 'dummy_2_class_dataset.csv' ein. Verwenden Sie dafür zB Pandas.\n",
    "# Wie viele Instanzen hat der Datensatz von jeder Klasse?\n",
    "# Plotten Sie die Datenpunkte in einem Scatterplot.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# read csv\n",
    "\n",
    "\n",
    "# count instances per class\n",
    "\n",
    "\n",
    "# plot data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### k-Nearest Neighbors\n",
    "\n",
    "k-Nearest Neighbors (kNN) ist eine Methode des maschinellen Lernens, die für Klassifikations- und Regressionsprobleme verwendet wird. Sie hat keine Parameter, die aus den Daten gelernt werden, sondern speichert die Trainingsdaten und verwendet diese für die Vorhersage. Wie aus der Theroie bekannt, wird für eine neue Beobachtung die Distanz zu allen Trainingsdaten berechnet. Die `k` nächsten Nachbarn werden dann verwendet, um die Klasse der neuen Beobachtung vorherzusagen. Die Anzahl der Nachbarn `k` ist neben der Distanz Metrik ein Hyperparameter, der festgelegt werden muss.\n",
    "\n",
    "Im folgenden Beispiel wollen wir die Klassifikation mit k-Nearest Neighbors (kNN) anhand des Iris-Datensatzes demonstrieren. Dazu laden wir zunächst den Datensatz und erstellen ein Pandas DataFrame:\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wollen nun die Daten in Trainings- und Testdaten aufteilen. Dazu verwenden wir die Funktion `train_test_split` aus dem Modul `model_selection`:\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[iris.feature_names].values, df['target'].values, random_state=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können nun ein kNN-Modell erstellen und trainieren:\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Anschließend können wir das Modell auf den Testdaten evaluieren:\n",
    "\n",
    "```python\n",
    "knn.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "Wir erhalten eine Genauigkeit von 97.4%. Um die Vorhersagen des Modells für die Testdaten auszugeben, verwenden wir die Funktion `predict`:\n",
    "```python\n",
    "knn.predict(X_test)\n",
    "```\n",
    "\n",
    "Die Wahrscheinlichkeiten für die einzelnen Klassen erhalten wir mit der Funktion `predict_proba`:\n",
    "```python\n",
    "knn.predict_proba(X_test)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufgabe:\n",
    "# Laden Sie den Datensatz 'dummy_2_class_dataset.csv' erneut.\n",
    "# Teilen Sie den Datensatz in einen Trainings- und Test-Teil auf. Verwenden Sie dafür die Funktion train_test_split aus sklearn.\n",
    "# Beachten Sie, dass diese Aufteilung zufällig passiert und Sie daher für reproduzierbare Ergebnisse einen festen seed definieren sollten (random_state=0).\n",
    "# Trainieren Sie einen kNN-Klassifikator mit k=3 auf dem Trainings-Teil des Datensatzes.\n",
    "# Testen Sie den Klassifikator auf dem Test-Teil des Datensatzes.\n",
    "# Variieren Sie den Parameter k und beobachten Sie die Auswirkungen auf die Klassifikationsleistung.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# read csv\n",
    "\n",
    "\n",
    "# split data into train and test set\n",
    "\n",
    "\n",
    "# train kNN classifier\n",
    "\n",
    "\n",
    "# test kNN classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufgabe 2:\n",
    "# Vor dem Anwenden der Klassifikatoren sollten Sie den Datensatz zunächst Normalisieren. \n",
    "# Berechnen Sie dafür den Mittelwert und die Standardabweichung für jedes Feature des Datensatzes.\n",
    "# Normalisieren Sie den Datensatz dann mit der Formel (x - mean) / std.\n",
    "# Trainieren Sie einen kNN-Klassifikator mit k=3 auf dem normalisierten Trainings-Teil des Datensatzes.\n",
    "# Testen Sie den Klassifikator auf dem normalisierten Test-Teil des Datensatzes.\n",
    "# Hat die Normalisierung einen Einfluss auf die Klassifikationsleistung?\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# read csv\n",
    "\n",
    "\n",
    "# normalize data\n",
    "\n",
    "\n",
    "# split data into train and test set\n",
    "\n",
    "\n",
    "# train kNN classifier\n",
    "\n",
    "\n",
    "# test kNN classifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Der Vorteil der dummy Datensätze ist, dass diese nur zwei Features haben und wir die Vorhersagen in einem zweidimensionalen Koordinatensystem visualisieren können. In der folgenden Zelle ist eine Funktion gegeben, die Sie für die Visalisierung der Vorhersagen verwenden können. Die Funktion nimmt als Argumente das trainierte Modell (clf), die Features (X) und die Zielvariablen (y) entgegen. Um die Funktion zu verwenden, müssen Sie die Zelle zunächst ausführen. Anschließend können Sie die Funktion wie folgt aufrufen:\n",
    "\n",
    "```python\n",
    "\n",
    "# visualize decision boundary\n",
    "plot_decision_bnoundary(knn, X_test, y_test, title='Test Data kNN')\n",
    "```\n",
    "\n",
    "<img src=\"images/sklearn/decision_boundary_knn.png\" width=\"550\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "def plot_decision_bnoundary(clf, X, y, title=None):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    f1,f2 = X[:,0],X[:,1]\n",
    "\n",
    "    x_min, x_max = f1.min() - 1, f1.max() + 1\n",
    "    y_min, y_max = f2.min() - 1, f2.max() + 1\n",
    "\n",
    "    resolution=0.02\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution), np.arange(y_min, y_max, resolution))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.6)\n",
    "\n",
    "    # plot the given data\n",
    "    scatter = ax.scatter(f1, f2, c=y, cmap=plt.cm.coolwarm, s=30)\n",
    "\n",
    "    # mark false predictions with a cross\n",
    "    y_pred = clf.predict(X)\n",
    "    false_pred = y_pred != y\n",
    "    ax.scatter(f1[false_pred], f2[false_pred], marker='x', c='r', s=40)\n",
    "\n",
    "\n",
    "    # plot legend\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.legend(*scatter.legend_elements(), loc=\"best\", title=\"Classes\", fontsize='small')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lassen Sie die Decision Boundary für verschiedene `k` ausgeben. \n",
    "# Was fällt Ihnen auf? Wie verändert sich die Decision Boundary? Welche Vor- und Nachteile hat kNN?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees\n",
    "Soll anstelle des kNN-Modells ein anderer Klassifikator verwendet werden, so muss nur die entsprechende Klasse importiert werden. Im folgenden Beispiel verwenden wir einen Decision Tree. Dazu importieren wir die Klasse `DecisionTreeClassifier` aus dem Modul `tree`:\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Random Forests\n",
    "Auch für Random Forests muss nur die entsprechende Klasse importiert werden. Im folgenden Beispiel verwenden wir einen Random Forest. Dazu importieren wir die Klasse `RandomForestClassifier` aus dem Modul `ensemble`:\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufgabe:\n",
    "# Wiederholen Sie die Aufgaben vom kNN-Beispiel mit einem Decision Tree bzw. Random Forest.\n",
    "# Wie verändert sich die Klassifikationsleistung?\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zusatzaufgabe\n",
    "Verwenden Sie nicht die Dummy Datensätze sondern den aus dem vorherigen Übungsblatt kennnengelernten Titanic-Datensatz. \n",
    "Beachten Sie, dass dieser Datensatz zunächst noch aufbereitet werden muss. So müssen beispielsweise die fehlenden Werte ersetzt werden oder die Samples mit fehlenden Werten entfernt werden. Auch müssen die kategorischen Variablen in numerische Variablen umgewandelt werden (zB Embarked, Sex). \n",
    "Normalisieren Sie anschließend alle nicht kategorischen Variablen. \n",
    "Teilen Sie den Datensatz in Trainings- und Testdaten auf.\n",
    "Nachdem der Datensatz aufbereitet wurde, können Sie die Klassifikation mit kNN, Decision Trees und Random Forests durchführen.\n",
    "Vergleichen Sie die Ergebnisse der einzelnen Klassifikatoren. Varrieren Sie die Hyperparameter der einzelnen Klassifikatoren und vergleichen Sie die Ergebnisse erneut. Variieren Sie auch die Anzahl der Features, die Sie für die Klassifikation verwenden. Wenn Sie nur zwei Merkmale verwenden, können Sie sich wieder die Decision Boundary ausgeben lassen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infineon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
