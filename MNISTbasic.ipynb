{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f71c223",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Simplistic attempts at an MNIST classifier: The Hello World of neural networks.\n",
    "\n",
    "The code here roughly reproduces what's done in 3Blue1Brown's introductory videos on neural networks. \n",
    "In case you have never seen those videos: you absolutely must! They are awesome! They can be found at: https://www.3blue1brown.com/topics/neural-networks \n",
    "\n",
    "Classifying digits is the \"Hello World\" of neural networks – a moderately difficult task that can be solved reasonably well even with a simple neural network. It's also possible to train on standard computers without having to wait forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f37d7c5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import ipywidgets as widgets\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0397801a-1a87-4a47-bc12-ee23e9879241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this if you want to train your model on the GPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6928e5c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# now, the following is cool: it lets us import functions that we define in other notebooks! \n",
    "%run PyTorch.ipynb import plot\n",
    "# (The drawback: we hardly ever write notebooks in a way that makes for good modules to import from; \n",
    "# specifically: the whole PyTorch notebook is run above, not only the cell that defines the plot function. \n",
    "# Calling %run can easily become slow.)\n",
    "# we use capture above to avoid seeing all the output that results from executing the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3f4dbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code reads a database of digits (the database is already included in the repository)\n",
    "# please don't look too closely at this code, I'm not proud of it.\n",
    "# minimally adapted from https://gist.github.com/akesling/5358964\n",
    "def read_mnist(dataset, path=\"data/MNIST/raw/\"):\n",
    "    if dataset == \"training\":\n",
    "        fname_img = os.path.join(path, \"train-images-idx3-ubyte\")\n",
    "        fname_lbl = os.path.join(path, \"train-labels-idx1-ubyte\")\n",
    "    elif dataset == \"testing\":\n",
    "        fname_img = os.path.join(path, \"t10k-images-idx3-ubyte\")\n",
    "        fname_lbl = os.path.join(path, \"t10k-labels-idx1-ubyte\")\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, \"rb\") as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        labels = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(fname_img, \"rb\") as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        images = np.multiply(\n",
    "            np.fromfile(fimg, dtype=np.uint8).reshape(len(labels), rows*cols),\n",
    "            1.0 / 255.0)\n",
    "\n",
    "    get_instance = lambda idx: (labels[idx], images[idx])\n",
    "\n",
    "    # Create an iterator which returns each image in turn\n",
    "    for i in range(len(labels)):\n",
    "        yield get_instance(i)\n",
    "        \n",
    "training = [(lbl, img) for (lbl, img) in read_mnist(\"training\")]\n",
    "testing = [(lbl, img) for (lbl, img) in read_mnist(\"testing\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15265891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b1f910b9974446b5a7297ae6dc4175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x00\\x00\\x00\\x00Wf\\x80H\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727ac8a2c8e647079b29917fe0fa7f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x00\\x00\\x00\\x00Wf\\x80H\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b5a961dd2342e7a44142cf26dd1a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x00\\x00\\x00\\x00Wf\\x80H\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e28538dc7674302837ae257bc36bad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x00\\x00\\x00\\x00Wf\\x80H\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f676195c11461c91edefa137c3ad0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x00\\x00\\x00\\x00Wf\\x80H\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62aab7a273f47a1bea77b60a68f19b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x00\\x00\\x00\\x00Wf\\x80H\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850b62bf60dd48db95f98bda1ab2bac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x00\\x00\\x00\\x00Wf\\x80H\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e12358493984015a534976b7a3f8cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x00\\x00\\x00\\x00Wf\\x80H\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74256ca9ced45d2b381856e432f8f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x00\\x00\\x00\\x00Wf\\x80H\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2328f0ad7414f2d81c006772cec52a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x00\\x00\\x00\\x00Wf\\x80H\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print some examples (pairs of target number and image)\n",
    "import io\n",
    "for i in range(10):\n",
    "    buffer = io.BytesIO()\n",
    "    PIL.Image.fromarray((256*training[i][1]).reshape(28, 28)).convert('L').save(buffer, format=\"PNG\")\n",
    "    display(training[i][0], widgets.Image(value=buffer.getvalue(),width=28,height=28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a6111-6c96-4811-9056-193e13b283eb",
   "metadata": {},
   "source": [
    "# A simple neural network for MNIST\n",
    "In MNIST we have grayscale images of digits, 28x28 pixels in size (as shown above). \n",
    "We will define a network with two inner (hidden) layers, each with 16 neurons, and an output layer with 10 neurons, one per digit.\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a1def90",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM1 = 16\n",
    "HIDDEN_DIM2 = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45dda405",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassify(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassify, self).__init__()\n",
    "        input_size = 28 * 28\n",
    "        self.W1 = nn.Parameter(nn.init.xavier_uniform_(torch.empty(input_size, HIDDEN_DIM1)))\n",
    "        self.b1 = nn.Parameter(nn.init.xavier_uniform_(torch.empty(1, HIDDEN_DIM1)))\n",
    "        self.W2 = nn.Parameter(nn.init.xavier_uniform_(torch.empty(HIDDEN_DIM1, HIDDEN_DIM2)))\n",
    "        self.b2 = nn.Parameter(nn.init.xavier_uniform_(torch.empty(1, HIDDEN_DIM2)))\n",
    "        self.W3 = nn.Parameter(nn.init.xavier_uniform_(torch.empty(HIDDEN_DIM2, 10)))\n",
    "        self.b3 = nn.Parameter(nn.init.xavier_uniform_(torch.empty(1, 10)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # erste innere Schicht:\n",
    "        h1 = nn.functional.sigmoid(x @ self.W1 + self.b1)\n",
    "        # zweite innere Schicht:\n",
    "        h2 = nn.functional.sigmoid(h1 @ self.W2 + self.b2)\n",
    "        # Ergebnisschicht:\n",
    "        activation = nn.functional.log_softmax(h2 @ self.W3 + self.b3, dim=1)\n",
    "        return activation.view(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763486b1-cda2-4b6c-9660-cb704db813d5",
   "metadata": {},
   "source": [
    "What's new here? \n",
    "\n",
    "We're not computing the outputs of individual neurons! Instead, we compute the weighted sum for all output neurons from all weighted input neurons in one matrix multiplication (which uses the ```@``` operator). We then add a bias *vector* (instead of just one bias value). The agreement is that sigmoid called on a vector yields the vector of the sigmoid operation performed on each individual value in the vector.\n",
    "\n",
    "Matrix operations really condense the computations needed for a full layer to <tt>a<sub>out</sub> = &sigma;(a<sub>in</sub> &middot; W + b)</tt>. Note again, that operator overloading makes this concise form possible. \n",
    "\n",
    "What's important to note is that we need to make sure that weight matrices (and bias vectors) are of the right sizes, check the sizes in the constructor.\n",
    "\n",
    "Also, note that we initialize the parameters in a certain way – one that was vital discovery in making deep learning actually work in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc25900-1675-43e1-b9b4-d5a988b99b34",
   "metadata": {},
   "source": [
    "# The training loop.\n",
    "The training is actually quite similar to what we did in the previous exercise. \n",
    "we over and over repeat the following:\n",
    "* have the network produce some output (**new:** there's now input involved!)\n",
    "* compute the desired output (**new:** this also comes from data!)\n",
    "* compute the model loss as the squared error between target and estimate\n",
    "* compute the gradients of the error wrt. all parameters\n",
    "* adjust errors a little bit in the right direction.\n",
    "\n",
    "We perform this repeatedly and every time with a different image. Once we're through with every image, we repeat, for a total of 5 repetitions (each repetition is called an *epoch*). \n",
    "After every epoch, we **test** the model on some validation data. In that way, we have a better chance at assessing the real-world performance of the model on new data, as compared to data that has been seen in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f16f533-cdef-4a01-a7d4-23a0365ff899",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 starting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50014/1939699684.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Kostenfunktion berechnen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m#loss  = nn.functional.nll_loss(output, torch.tensor(cls).long().to(device))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mplot_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# See https://github.com/pytorch/pytorch/issues/75462\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "classify = MNISTClassify().float().to(device)\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(classify.parameters(), lr = learning_rate)\n",
    "#optimizer = torch.optim.Adam(classify.parameters())\n",
    "#loss = nn.CrossEntropyLoss\n",
    "dev_time = 0.0\n",
    "plot_data = defaultdict(lambda : [])\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(5):\n",
    "    print((\"Epoch {} starting\".format(epoch+1)))\n",
    "    random.shuffle(training)\n",
    "    # train model:\n",
    "    recent_errors = []\n",
    "    classify.train(True)\n",
    "    for cls, img in training:\n",
    "        optimizer.zero_grad()\n",
    "        # Bild als Eingabe aufbereiten\n",
    "        x = torch.tensor(img).float().to(device)\n",
    "        # Outputaktivierung berechnen\n",
    "        output = classify(x)\n",
    "        # \"idealen\" Output (überall 0, nur gewählte Klasse ist 1)\n",
    "        target = torch.zeros(10).long().to(device)\n",
    "        target[cls] = 1.0\n",
    "        # Kostenfunktion berechnen\n",
    "        loss = torch.sum((output - target) ** 2)\n",
    "        #loss  = nn.functional.nll_loss(output, torch.tensor(cls).long().to(device))\n",
    "        plot_data[\"error\"].append(loss.item())\n",
    "        recent_errors.append(loss.item())\n",
    "        if len(recent_errors) > 100:\n",
    "            recent_errors = recent_errors[-100:-1] # keep only the last part\n",
    "        plot_data[\"recent avg. error\"].append(sum(recent_errors) / len(recent_errors))\n",
    "        # Kosten den einzelnen Parametern zuordnen\n",
    "        loss.backward()\n",
    "        # Parameter entsprechend anpassen\n",
    "        optimizer.step()\n",
    "    classify.train(False)\n",
    "    \n",
    "    # test performance of classifier:\n",
    "    confusion = [[0 for _ in range(10)] for _ in range(10)]\n",
    "    correct = 0\n",
    "    dev_start = time.time()\n",
    "    for cls, img in testing:\n",
    "        x = torch.tensor(img).float().to(device)\n",
    "        output = classify(x)\n",
    "        prediction = int(torch.topk(output, 1)[1].item())\n",
    "        if cls == prediction:\n",
    "            correct += 1\n",
    "        confusion[prediction][cls] += 1\n",
    "    dev_end = time.time()\n",
    "    acc = float(correct) / len(testing)\n",
    "    dev_time += dev_end - dev_start\n",
    "    print((\"Held out accuracy {} ({} instances/sec)\".format(\n",
    "        acc, len(testing) / (dev_end - dev_start))))\n",
    "    print('   ' + ''.join(('T'+str(x)).ljust(6) for x in range(10)))\n",
    "    for p, row in enumerate(confusion):\n",
    "        s = 'P' + str(p) + ' '\n",
    "        s += ''.join(str(col).ljust(6) for col in row)\n",
    "        print(s)\n",
    "\n",
    "    end = time.time()\n",
    "    print((\"instances per sec: {}\".format(\n",
    "                ((epoch+1) * len(training)) / (end - start - dev_time))))\n",
    "\n",
    "plot(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c96d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell provides an alternative to the above implementation. It should do precisely the same but defers the actual implementation to \n",
    "# sub-modules (nn.Linear).\n",
    "\n",
    "class MNISTClassify(nn.Module):\n",
    "    \"\"\"the same but using the built-in modules for densely connected feed-forward layers\"\"\"\n",
    "    def __init__(self):\n",
    "        super(MNISTClassify, self).__init__()\n",
    "        input_size = 28 * 28\n",
    "        self.l1 = nn.Linear(input_size, HIDDEN_DIM1)\n",
    "        self.l2 = nn.Linear(HIDDEN_DIM1, HIDDEN_DIM2)\n",
    "        self.l3 = nn.Linear(HIDDEN_DIM2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.log_softmax(self.l3(nn.functional.sigmoid(self.l2(nn.functional.sigmoid(self.l1(x))))), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93016f8a-9f13-4ce2-a31f-ea6ec9b1000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class MNISTClassify(nn.Module):\n",
    "    \"\"\"works much better with CNNs.\n",
    "    shamefully stolen/borrowed from https://github.com/mbjoseph/pytorch-mnist/blob/master/cnn-mnist.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MNISTClassify, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=5)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x, isTrain=False):\n",
    "        x = x.reshape((1,1,28,28))\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=-1)\n",
    "        return x.view((10))\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d96d0bd6-8c0b-4646-880c-db1016a6ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class MNISTClassify(nn.Module):\n",
    "    \"\"\"works much better with CNNs.\n",
    "    shamefully stolen/borrowed from https://github.com/mbjoseph/pytorch-mnist/blob/master/cnn-mnist.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MNISTClassify, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 120)\n",
    "        self.fc1_drop = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x, isTrain=False):\n",
    "        x = x.reshape((1,1,28,28))\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1_drop(self.fc1(x)))\n",
    "        x = F.log_softmax(self.fc2(x), dim=-1)\n",
    "        return x.view((10))\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ff34a-0a59-49cd-bd62-8284d817ce6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
